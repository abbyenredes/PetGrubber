# PetGrubber
El webscrapper que todo amante de los animales debe conocer ğŸ¦´
 
<details>
  <summary>Â¿Que voy a encontrarme?</summary>
  <ol>
    <li>
      <a href="#Â¿QuÃ©-es-PetGrubber">Â¿QuÃ© es PetGrubber?</a>
      <ul>
        <li><a href="#CaracterÃ­sticas"> CaracterÃ­sticas</a></li>
       <li><a href="#Mis-recursos">Mis recursos</a></li>
      </ul>
    </li>
    <li><a href="#Como-usar-PetGrubber">Como usar PetGrubber</a></li>
   <li><a href="#Mi-paso-a-paso">Mi paso a paso</a></li>
     <li><a href="#ContribuciÃ³n">ContribuciÃ³n</a></li>
  </ol>
</details>

## Â¿QuÃ© es PetGrubber?
 Â¿Cansado de pasar horas buscando los mejores productos para tu mascota?
 Esta herramienta es para ti, optimiza esa busqueda con un solo click, enseÃ±andote los productos con mejor valoraciÃ³n y permitiendote elegir lo mejor para tu compaÃ±ero peludo.  

 ## CaracterÃ­sticas 

1. **AutomatizaciÃ³n del Navegador**  
   - Utiliza `undetected_chromedriver` de **Selenium** para evitar bloqueos por detecciÃ³n de bots y cargar pÃ¡ginas dinÃ¡micas correctamente.  
   - Permite interactuar con la pÃ¡gina, como aceptar cookies automÃ¡ticamente.  

2. **ExtracciÃ³n de Datos con BeautifulSoup**  
   - Analiza el cÃ³digo HTML con `BeautifulSoup` para extraer informaciÃ³n clave como el **nombre, marca, precio y valoraciÃ³n** del producto.  
   - Implementa manejo de errores para evitar fallos si un elemento no estÃ¡ disponible.  

3. **Almacenamiento de Datos en CSV**  
   - Usa `pandas` para organizar los datos en un **DataFrame** y exportarlos en formato CSV (`productos.csv`).  
   - Permite un formato estructurado y fÃ¡cil de analizar para futuras consultas.  

4. **Eficiencia y Escalabilidad**  
   - Implementa **WebDriverWait** para optimizar la carga de elementos sin esperas innecesarias.  
   - Puede adaptarse fÃ¡cilmente para extraer informaciÃ³n de mÃºltiples productos o diferentes pÃ¡ginas web.  

 
## Mis recursos
* Tienda demo para practicar selenium: [saucedemo](https://www.saucedemo.com/)
* Libreria demo para practicar webscraping:  [books.toscrape](https://books.toscrape.com/)
* Video del que saque la logica de selenium: [Webscrapping idealista](https://www.youtube.com/watch?v=JKwfzexrQS0&ab_channel=JaviDataScience)
* Tutorial de pandas: [pandas](https://www.datacamp.com/es/tutorial/pandas)
* Guia sobre webscrepping: [La guÃ­a definitiva para Web Scraping](https://www.rapidseedbox.com/es/blog/web-scraping)

## Como usar PetGrubber
> [!WARNING]
> este programa esta creado con la version de python3.10

### ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### 1ï¸âƒ£ Clonar el repositorio y entrar

```textplain
git clone https://github.com/tu-API
cd tu-API
```

### 2ï¸âƒ£ Descarga el entorno virtual:
âš ï¸ linux/mac
```textplain
python3.10 -m venv .venvv
```
âš ï¸ windows
```texrplain
python3.10 -m venv .venv
```

### 3ï¸âƒ£ Inicia el entorno virtual:
âš ï¸ linux/mac
```textplain
source .venv/bin/activate
```
âš ï¸ windows
```textplain
.venv\Scripts\activate
```

### 4ï¸âƒ£ Descarga las siguientes dependencias:
```textplain
 pip install -r requirements.txt
```

> [!TIP]
> Con `pip list` puedes visualizar todas las dependencias descargadas.

### 5ï¸âƒ£ Ejecuta el scrapper 

```textplain
python3 main.py
```

### 6ï¸âƒ£ Disfruta del resultado

![video_demo](https://github.com/abbyenredes/PetGrubber/blob/main/img/scraper.gif)

### Mi paso a paso

1ï¸âƒ£ Â¿Cual es mi finalidad?

Primero pense en que datos gustarÃ­a scrappear, me gusta el mundo de los animales asÃ­ que me parecio una gran oportunidad aportar valor y una de mis metas fue [tiendanimal](https://www.tiendanimal.es/), visualice su [robots.txt](https://www.tiendanimal.es/robots.txt) y me di cuenta que todo en su **User-agent** estaba en **Disallow** lo que significa que esta pÃ¡gina no desea ser scrappeada. Son pocas las pÃ¡ginas que realmente lo permitan pero eso no me freno a continuar.

2ï¸âƒ£ Definiendo los datos:
SegÃºn mi experiencia como asistente veterinario y tutora de mascotas, los datos revelantes para extraer en esta tienda son:

* Nombre del producto
* Marca
* Precio
* CalificaciÃ³n (este para mi es un dato muy importante, quiero darle calidad de vida a mis mascotas sin importar si es mas costoso)

3ï¸âƒ£ Examinar donde estan esos datos y para ello use la opcion de inspeccionar pÃ¡gina.

![video-inspeccionar](https://github.com/abbyenredes/PetGrubber/blob/main/img/inspeccionar.gif)

4ï¸âƒ£ Definir que biblioteca de websrappping usar.

| LibrerÃ­a        | DescripciÃ³n | Pros | Contras |
|----------------|------------|------|---------|
| **BeautifulSoup** | Analiza y extrae datos de HTML y XML de forma sencilla. | âœ… FÃ¡cil de usar, flexible, ideal para proyectos pequeÃ±os y medianos. | âŒ No descarga pÃ¡ginas web, necesita `requests` o `urllib`. Lento para grandes volÃºmenes de datos. |
| **Scrapy**      | Framework completo para web scraping, permite rastrear y extraer datos de mÃºltiples pÃ¡ginas de manera eficiente. | âœ… RÃ¡pido, eficiente, maneja peticiones asÃ­ncronas, tiene soporte integrado para middlewares y almacenamiento de datos. | âŒ Curva de aprendizaje mÃ¡s alta, puede ser excesivo para tareas simples. |
| **Requests**    | LibrerÃ­a para hacer peticiones HTTP de manera sencilla y obtener el HTML de las pÃ¡ginas web. | âœ… FÃ¡cil de usar, permite manejar sesiones y autenticaciones. | âŒ No parsea HTML ni interactÃºa con JavaScript. Se usa junto con BeautifulSoup o lxml. |
| **Selenium**    | AutomaciÃ³n de navegadores, permite interactuar con pÃ¡ginas web dinÃ¡micas. | âœ… Soporta JavaScript y pÃ¡ginas dinÃ¡micas, permite interactuar con formularios y botones. | âŒ Lento en comparaciÃ³n con Scrapy, requiere un navegador instalado. |

En mi caso empece creando un script bÃ¡sico con  **BeautifulSoup**  **Requests** y me encontre mi primer problema, mi versiÃ³n de python no era apta para **BeautifulSoup**

```shell
PetGrubber on î‚  main [?] via ğŸ v3.13.0a3 (.venv) 
â¯ python3 -c "from bs4 import BeautifulSoup; print(BeautifulSoup('<html></html>', 'html.parser'))"
Segmentation fault (core dumped)
```
DespuÃ©s de investigar y probar que habia descargado correctamente, borrar y volver a crear mi entorno visual di que era por mi version de python. Mi solucion:

Borrar nuevamente mi entorno virtual:
```textplain
rm -rf .venv
```
instalarlo con python3.10
```textplain
python3.10 -m venv .venv
```
Descargar mis dependencias
```textplain
pip install beautifulsoup4 lxml requests
```
âœ”ï¸ probar lo que llevaba de script

Â¿Por quÃ© con esta versiÃ³n y no con otra?
pues probe desde 3.12, 3.11 y solo con la 3.10 funciono.

```shell
PetGrubber on î‚  main [?] via ğŸ v3.13.0a3 (.venv) 
â¯ python3.12 -c "from bs4 import BeautifulSoup; print(BeautifulSoup('<html></html>', 'html.parser'))"
Command 'python3.12' not found, did you mean:
  command 'python3.10' from deb python3.10 (3.10.12-1~22.04.9)
  command 'python3.11' from deb python3.11 (3.11.0~rc1-1~22.04)
Try: sudo apt install <deb name>
```
Con esta version **BeautifulSoup** es estable y compatible

```shell
PetGrubber on î‚  main [?] via ğŸ v3.13.0a3 (.venv) 
â¯ python3.10 -c "from bs4 import BeautifulSoup; print(BeautifulSoup('<html></html>', 'html.parser'))"
<html></html>
```

Pactique un poco con esta pÃ¡gina: [books.toscrape](https://books.toscrape.com/) ya que no queria ser baneada por parecer un bot. En esta mi misiÃ³n era aprender a hacer scrapping por puntuacion y solo descargar los libros con la mejor puntuaciÃ³n, esta es mi prÃ¡ctica:

```python
import bs4
import requests

url_base = 'https://books.toscrape.com/catalogue/page-{}.html'

# lista de titulos con 4 o 5 estrellas
titulos_rating_alto = []

# iterar paginas
for pagina in range(1, 51):

    url_pagina = url_base.format(pagina)
    resultado = requests.get(url_pagina)
    sopa = bs4.BeautifulSoup(resultado.text, 'lxml')

    # seleccionar datos de los libros
    libros = sopa.select('.product_pod')

    for libro in libros:

        # chequear que tengan 4 o 5 estrellas
        if len(libro.select('.star-rating.Four')) != 0 or len(libro.select('.star-rating.Five')) != 0:
            titulo_libro = libro.select('a')[1]['title']
            titulos_rating_alto.append(titulo_libro)

# ver libros 4 u 5 estrellas en consola
for t in titulos_rating_alto:
    print(t)
```
InformaciÃ³n Ãºtil sobre BeautifulSoup:
| MÃ©todo                        | DescripciÃ³n                                                                               | Ejemplo                                                               |
|--------------------------------|-------------------------------------------------------------------------------------------|-----------------------------------------------------------------------|
| `find(name, attrs)`           | Encuentra el primer elemento que coincide con la etiqueta y atributos dados.              | `soup.find('div', class_='contenido')`                                |
| `find_all(name, attrs)`       | Encuentra todos los elementos que coinciden con la etiqueta y atributos dados.            | `soup.find_all('p', class_='texto')`                                  |
| `select(css_selector)`        | Encuentra elementos usando selectores CSS.                                                | `soup.select('div#id_contenedor p.texto')`                            |
| `get_text()`                  | Extrae el texto de un elemento, eliminando etiquetas HTML.                                | `elemento.get_text()`                                                 |
| `get(attribute)`              | Obtiene el valor de un atributo de un elemento.                                           | `elemento.get('href')`                                                |
| `parent`                      | Obtiene el elemento padre del elemento actual.                                            | `elemento.parent`                                                     |
| `find_parent(name, attrs)`    | Encuentra el primer elemento padre que coincide con el nombre y atributos dados.          | `elemento.find_parent('div')`                                         |
| `find_parents(name, attrs)`   | Encuentra todos los elementos padres que coinciden con el nombre y atributos dados.       | `elemento.find_parents('div')`                                        |
| `children`                    | Retorna los elementos hijos directos de un elemento.                                      | `for hijo in elemento.children:`                                      |
| `descendants`                 | Retorna todos los elementos descendientes de un elemento.                                | `for descendiente in elemento.descendants:`                          |
| `next_sibling`                | Obtiene el siguiente elemento en el mismo nivel.                                         | `elemento.next_sibling`                                               |
| `previous_sibling`            | Obtiene el elemento anterior en el mismo nivel.                                          | `elemento.previous_sibling`                                           |
| `decompose()`                 | Elimina un elemento del Ã¡rbol de BeautifulSoup.                                          | `elemento.decompose()`                                                |
| `replace_with(nuevo_elemento)`| Reemplaza un elemento con otro.                                                           | `elemento.replace_with('Nuevo contenido')`                            |
| `prettify()`                  | Devuelve el HTML estructurado y bien formateado.                                         | `print(soup.prettify())`                                              |

> [!NOTE]
> Para este proyecto use los mÃ©todos:
> 
> `bs(html, 'lxml')` para analizar el HTML de la pÃ¡gina y extraer datos.
> 
> `soup.find('div', class_='contenido')` para extraer las distintas etiquetas HTML.

Con esta base implemente mi primera versiÃ³n pero me tope con varios inconvenientes que me hicieron elegir usar **Selenium** y el primero fue el manejo de cookies y una implementaciÃ³n para no ser baneado facilmente de las pÃ¡ginas. Aparte que la pÃ¡gina que queria scrappear es dinamica.
AquÃ­ los diferentes mÃ©todos que tiene:

| MÃ©todo                     | DescripciÃ³n                                                                 | Ejemplo                                                                 |
|----------------------------|-----------------------------------------------------------------------------|-------------------------------------------------------------------------|
| `get(url)`                 | Abre una pÃ¡gina web en el navegador.                                        | `driver.get('https://example.com')`                                    |
| `find_element()`           | Encuentra un elemento en la pÃ¡gina usando un selector.                     | `driver.find_element(By.ID, 'elemento_id')`                            |
| `find_elements()`          | Encuentra mÃºltiples elementos en la pÃ¡gina.                                | `driver.find_elements(By.CLASS_NAME, 'clase_elemento')`                |
| `click()`                  | Hace clic en un elemento.                                                   | `elemento.click()`                                                     |
| `send_keys(texto)`         | Escribe texto en un campo de entrada.                                       | `elemento.send_keys('Hola mundo')`                                     |
| `clear()`                  | Borra el contenido de un campo de entrada.                                 | `elemento.clear()`                                                     |
| `submit()`                 | EnvÃ­a un formulario.                                                        | `elemento.submit()`                                                    |
| `get_attribute(nombre)`    | Obtiene el valor de un atributo del elemento.                              | `elemento.get_attribute('href')`                                       |
| `is_displayed()`           | Verifica si un elemento es visible.                                        | `elemento.is_displayed()`                                              |
| `is_enabled()`             | Verifica si un elemento estÃ¡ habilitado.                                   | `elemento.is_enabled()`                                                |
| `is_selected()`            | Verifica si un checkbox o radio estÃ¡ seleccionado.                        | `elemento.is_selected()`                                               |
| `execute_script(script)`   | Ejecuta cÃ³digo JavaScript en la pÃ¡gina.                                   | `driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")` |
| `switch_to.frame(frame)`   | Cambia a un iframe en la pÃ¡gina.                                          | `driver.switch_to.frame('nombre_iframe')`                              |
| `switch_to.default_content()` | Sale del iframe y vuelve al contenido principal.                    | `driver.switch_to.default_content()`                                   |
| `implicitly_wait(segundos)` | Espera implÃ­cita antes de que falle una bÃºsqueda de elemento.          | `driver.implicitly_wait(10)`                                           |
| `WebDriverWait(driver, tiempo).until()` | Espera hasta que una condiciÃ³n se cumpla.                 | `WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'elemento_id')))` |
| `back()`                   | Navega a la pÃ¡gina anterior en el historial.                              | `driver.back()`                                                        |
| `forward()`                | Navega a la pÃ¡gina siguiente en el historial.                             | `driver.forward()`                                                     |
| `refresh()`                | Recarga la pÃ¡gina actual.                                                  | `driver.refresh()`                                                     |
| `close()`                  | Cierra la pestaÃ±a actual del navegador.                                   | `driver.close()`                                                       |
| `quit()`                   | Cierra completamente el navegador.                                        | `driver.quit()`                                                        |
> [!NOTE]
> AquÃ­ los mÃ©todos que use yo:
>
> `undetected_chromedriver.Chrome()` para iniciar un navegador y evitar bloqueos por detecciÃ³n de bots.
>
> `browser.get(url)`  para abir la pÃ¡gina web dada.
>
> `WebDriverWait(browser, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id="onetrust-accept-btn-handler"]'))).click()` y mi favorito ya que espera hasta que aparezca el botÃ³n de cookies y hace clic en Ã©l.
>
> `browser.page_source` para obtener el cÃ³digo HTML de la pÃ¡gina.
>
> `browser.quit()` para Cerrar el navegador cuando termina el scraping.

5ï¸âƒ£ Guardado de esos datos

Al principio necesitaba ver que los datos extraidos eran los correctos y use la funciÃ³n `printf`, sin embargo eso no es lo Ã³ptimo probe con json pero la verdad esteticamente no me gusto y por ello elegÃ­ Pandas ya que me exporta los datos en un documento cvs de una forma dinamica.
aqui dejo un resumen de sus distintos mÃ³dulos:

| MÃ©todo               | DescripciÃ³n                                                                                      | Ejemplo                                                                                           |
|----------------------|--------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|
| `read_csv()`         | Importa datos desde un archivo CSV a un DataFrame.                                               | `df = pd.read_csv('datos.csv')`                                                                   |
| `head()`             | Muestra las primeras n filas del DataFrame (por defecto, n=5).                                   | `df.head()`                                                                                       |
| `tail()`             | Muestra las Ãºltimas n filas del DataFrame (por defecto, n=5).                                    | `df.tail()`                                                                                       |
| `info()`             | Proporciona un resumen conciso del DataFrame, incluyendo el nÃºmero de entradas y tipos de datos. | `df.info()`                                                                                       |
| `describe()`         | Genera estadÃ­sticas descriptivas para las columnas numÃ©ricas del DataFrame.                      | `df.describe()`                                                                                   |
| `sort_values()`      | Ordena el DataFrame segÃºn los valores de una o mÃ¡s columnas.                                     | `df.sort_values(by='columna', ascending=True)`                                                    |
| `groupby()`          | Agrupa el DataFrame utilizando una o mÃ¡s columnas y permite aplicar funciones de agregaciÃ³n.     | `df.groupby('columna').mean()`                                                                    |
| `drop_duplicates()`  | Elimina filas duplicadas del DataFrame.                                                          | `df.drop_duplicates()`                                                                            |
| `rename()`           | Cambia el nombre de las columnas o filas del DataFrame.                                          | `df.rename(columns={'viejo_nombre': 'nuevo_nombre'})`                                             |
| `value_counts()`     | Cuenta la cantidad de ocurrencias Ãºnicas de valores en una serie o columna.                      | `df['columna'].value_counts()`                                                                    |
| `merge()`            | Combina dos DataFrames basÃ¡ndose en una clave comÃºn.                                             | `pd.merge(df1, df2, on='columna_clave')`                                                          |
| `pivot_table()`      | Crea una tabla dinÃ¡mica que resume los datos segÃºn las variables especificadas.                  | `df.pivot_table(values='columna_valor', index='columna_indice', columns='columna_columnas')`      |
| `fillna()`           | Rellena los valores NaN (nulos) con un valor especÃ­fico.                                         | `df.fillna(valor=0)`                                                                              |
| `dropna()`           | Elimina las filas o columnas que contienen valores nulos.                                        | `df.dropna()`                                                                                     |
| `apply()`            | Aplica una funciÃ³n a lo largo de un eje del DataFrame (filas o columnas).                        | `df['columna'].apply(funcion)`                                                                    |
| `astype()`           | Convierte el tipo de datos de una o mÃ¡s columnas.                                                | `df['columna'] = df['columna'].astype(int)`                                                       |
| `set_index()`        | Establece una columna especÃ­fica como el Ã­ndice del DataFrame.                                   | `df.set_index('columna')`                                                                         |
| `reset_index()`      | Restablece el Ã­ndice del DataFrame al valor predeterminado.                                      | `df.reset_index()`                                                                                |
| `loc[]`              | Accede a un grupo de filas y columnas por etiquetas o una matriz booleana.                       | `df.loc[filas, columnas]`                                                                         |
| `iloc[]`             | Accede a un grupo de filas y columnas por Ã­ndices enteros.                                       | `df.iloc[filas, columnas]`                                                                        |

> [|NOTE]
> Estos son los pandas que use:
> 
> `pd.DataFrame([data])` para conviertir los datos extraÃ­dos en un DataFrame.
>
> `df.to_csv('productos.csv', index=False, sep=';', encoding='utf-8')` para guardar los datos en un archivo CSV.

6ï¸âƒ£ Implemente una bateria de test

![test]()

---

## ğŸ¤ ContribuciÃ³n  

Â¡Las contribuciones son bienvenidas! Para contribuir:  

1. Haz un fork del repositorio.
   
3. Crea una nueva rama:
    
   ```sh
   git checkout -b feature/nueva-funcionalidad
   ```
   
4. Realiza tus cambios y haz commit:
   
  ```sh
git commit -m "AÃ±adir nueva funcionalidad"
```

4. EnvÃ­a un pull request ğŸš€.
   
---
## ğŸš€ Â¡Gracias por usar PetGrubber! Si tienes preguntas, crea un issue en el repositorio o contÃ¡ctanos.
