# PetGrubber
El webscrapper que todo amante de los animales debe conocer ü¶¥
 ## Acerca del proyecto:
 
 ¬øCansado de pasar horas buscando los mejores productos para tu mascota?

 Esta herramienta es para ti, optimiza esa busqueda con un solo click, ense√±andote los productos con mejor valoraci√≥n y permitiendote elegir lo mejor para tu compa√±ero peludo. 
 
## Mis recursos
* Tienda demo para practicar selenium: [saucedemo](https://www.saucedemo.com/)
* Libreria demo para practicar webscraping:  [books.toscrape](https://books.toscrape.com/)
* Video del que saque la logica de selenium: [Webscrapping idealista](https://www.youtube.com/watch?v=JKwfzexrQS0&ab_channel=JaviDataScience)
* Tutorial de pandas: [pandas](https://www.datacamp.com/es/tutorial/pandas)
* Guia sobre webscrepping: [La gu√≠a definitiva para Web Scraping](https://www.rapidseedbox.com/es/blog/web-scraping)

## Como usar PetGrubber
> [!WARNING]
> este programa esta creado con la version de python3.10

### üöÄ Instalaci√≥n y Configuraci√≥n

### 1Ô∏è‚É£ Clonar el repositorio y entrar

```textplain
git clone https://github.com/tu-API
cd tu-API
```

### 2Ô∏è‚É£ Descarga el entorno virtual:
‚ö†Ô∏è linux/mac
```textplain
python3.10 -m venv .venvv
```
‚ö†Ô∏è windows
```texrplain
python3.10 -m venv .venv
```

### 3Ô∏è‚É£ Inicia el entorno virtual:
‚ö†Ô∏è linux/mac
```textplain
source .venv/bin/activate
```
‚ö†Ô∏è windows
```textplain
.venv\Scripts\activate
```

### 4Ô∏è‚É£ Descarga las siguientes dependencias:
```textplain
 pip install -r requirements.txt
```

> [!TIP]
> Con `pip list` puedes visualizar todas las dependencias descargadas.

### 5Ô∏è‚É£ Ejecuta el scrapper 

```textplain
python3 main.py
```

### Disfruta del resultado

![video_demo]()

### Mi paso a paso

1Ô∏è‚É£ ¬øCual es mi finalidad?

Primero pense en que datos gustar√≠a scrappear, me gusta el mundo de los animales as√≠ que me parecio una gran oportunidad aportar valor y una de mis metas fue [tiendanimal](https://www.tiendanimal.es/), visualice su [robots.txt](https://www.tiendanimal.es/robots.txt) y me di cuenta que todo en su **User-agent** estaba en **Disallow** lo que significa que esta p√°gina no desea ser scrappeada. Son pocas las p√°ginas que realmente lo permitan pero eso no me freno a continuar.

2Ô∏è‚É£ Definiendo los datos:
Seg√∫n mi experiencia como asistente veterinario y tutora de mascotas, los datos revelantes para extraer en esta tienda son:

* Nombre del producto
* Marca
* Precio
* Calificaci√≥n (este para mi es un dato muy importante, quiero darle calidad de vida a mis mascotas sin importar si es mas costoso)

3Ô∏è‚É£ Examinar donde estan esos datos y para ello use la opcion de inspeccionar p√°gina.

![video-inspeccionar]()

4Ô∏è‚É£ Definir que biblioteca de websrappping usar.

| Librer√≠a        | Descripci√≥n | Pros | Contras |
|----------------|------------|------|---------|
| **BeautifulSoup** | Analiza y extrae datos de HTML y XML de forma sencilla. | ‚úÖ F√°cil de usar, flexible, ideal para proyectos peque√±os y medianos. | ‚ùå No descarga p√°ginas web, necesita `requests` o `urllib`. Lento para grandes vol√∫menes de datos. |
| **Scrapy**      | Framework completo para web scraping, permite rastrear y extraer datos de m√∫ltiples p√°ginas de manera eficiente. | ‚úÖ R√°pido, eficiente, maneja peticiones as√≠ncronas, tiene soporte integrado para middlewares y almacenamiento de datos. | ‚ùå Curva de aprendizaje m√°s alta, puede ser excesivo para tareas simples. |
| **Requests**    | Librer√≠a para hacer peticiones HTTP de manera sencilla y obtener el HTML de las p√°ginas web. | ‚úÖ F√°cil de usar, permite manejar sesiones y autenticaciones. | ‚ùå No parsea HTML ni interact√∫a con JavaScript. Se usa junto con BeautifulSoup o lxml. |
| **Selenium**    | Automaci√≥n de navegadores, permite interactuar con p√°ginas web din√°micas. | ‚úÖ Soporta JavaScript y p√°ginas din√°micas, permite interactuar con formularios y botones. | ‚ùå Lento en comparaci√≥n con Scrapy, requiere un navegador instalado. |

En mi caso empece creando un script b√°sico con  **BeautifulSoup**  **Requests** y me encontre mi primer problema, mi versi√≥n de python no era apta para **BeautifulSoup**

```shell
PetGrubber on ÓÇ† main [?] via üêç v3.13.0a3 (.venv) 
‚ùØ python3 -c "from bs4 import BeautifulSoup; print(BeautifulSoup('<html></html>', 'html.parser'))"
Segmentation fault (core dumped)
```
Despu√©s de investigar y probar que habia descargado correctamente, borrar y volver a crear mi entorno visual di que era por mi version de python. Mi solucion:

Borrar nuevamente mi entorno virtual:
```textplain
rm -rf .venv
```
instalarlo con python3.10
```textplain
python3.10 -m venv .venv
```
Descargar mis dependencias
```textplain
pip install beautifulsoup4 lxml requests
```
‚úîÔ∏è probar lo que llevaba de script

¬øPor qu√© con esta versi√≥n y no con otra?
pues probe desde 3.12, 3.11 y solo con la 3.10 funciono.

```shell
PetGrubber on ÓÇ† main [?] via üêç v3.13.0a3 (.venv) 
‚ùØ python3.12 -c "from bs4 import BeautifulSoup; print(BeautifulSoup('<html></html>', 'html.parser'))"
Command 'python3.12' not found, did you mean:
  command 'python3.10' from deb python3.10 (3.10.12-1~22.04.9)
  command 'python3.11' from deb python3.11 (3.11.0~rc1-1~22.04)
Try: sudo apt install <deb name>
```
Con esta version **BeautifulSoup** es estable y compatible

```shell
PetGrubber on ÓÇ† main [?] via üêç v3.13.0a3 (.venv) 
‚ùØ python3.10 -c "from bs4 import BeautifulSoup; print(BeautifulSoup('<html></html>', 'html.parser'))"
<html></html>
```
Pactique un poco con esta p√°gina: [books.toscrape](https://books.toscrape.com/) ya que no queria ser baneada por parecer un bot. En esta mi misi√≥n era aprender a hacer scrapping por puntuacion y solo descargar los libros con la mejor puntuaci√≥n, esta es mi pr√°ctica:

```python
import bs4
import requests

url_base = 'https://books.toscrape.com/catalogue/page-{}.html'

# lista de titulos con 4 o 5 estrellas
titulos_rating_alto = []

# iterar paginas
for pagina in range(1, 51):

    url_pagina = url_base.format(pagina)
    resultado = requests.get(url_pagina)
    sopa = bs4.BeautifulSoup(resultado.text, 'lxml')

    # seleccionar datos de los libros
    libros = sopa.select('.product_pod')

    for libro in libros:

        # chequear que tengan 4 o 5 estrellas
        if len(libro.select('.star-rating.Four')) != 0 or len(libro.select('.star-rating.Five')) != 0:
            titulo_libro = libro.select('a')[1]['title']
            titulos_rating_alto.append(titulo_libro)

# ver libros 4 u 5 estrellas en consola
for t in titulos_rating_alto:
    print(t)
```

Con esta base implemnte mi primera versi√≥n pero me tope con varios inconvenientes que me hicieron elegir usar **Selenium** y el primero fue el manejo de cookies y una implementaci√≥n  para no ser baneado facilmente de las p√°ginas.

5Ô∏è‚É£ Guardado de esos datos

Al principio necesitaba ver que los datos extraidos eran los correctos y use la funci√≥n `printf`, sin embargo eso no es lo √≥ptimo probe con json pero la verdad esteticamente no me gusto y por ello eleg√≠ Pandas ya que me exporta los datos en un documento cvs de una forma dinamica.
aqui dejo un resumen de sus distintos m√≥dulos:

| M√©todo               | Descripci√≥n                                                                                      | Ejemplo                                                                                           |
|----------------------|--------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|
| `read_csv()`         | Importa datos desde un archivo CSV a un DataFrame.                                               | `df = pd.read_csv('datos.csv')`                                                                   |
| `head()`             | Muestra las primeras n filas del DataFrame (por defecto, n=5).                                   | `df.head()`                                                                                       |
| `tail()`             | Muestra las √∫ltimas n filas del DataFrame (por defecto, n=5).                                    | `df.tail()`                                                                                       |
| `info()`             | Proporciona un resumen conciso del DataFrame, incluyendo el n√∫mero de entradas y tipos de datos. | `df.info()`                                                                                       |
| `describe()`         | Genera estad√≠sticas descriptivas para las columnas num√©ricas del DataFrame.                      | `df.describe()`                                                                                   |
| `sort_values()`      | Ordena el DataFrame seg√∫n los valores de una o m√°s columnas.                                     | `df.sort_values(by='columna', ascending=True)`                                                    |
| `groupby()`          | Agrupa el DataFrame utilizando una o m√°s columnas y permite aplicar funciones de agregaci√≥n.     | `df.groupby('columna').mean()`                                                                    |
| `drop_duplicates()`  | Elimina filas duplicadas del DataFrame.                                                          | `df.drop_duplicates()`                                                                            |
| `rename()`           | Cambia el nombre de las columnas o filas del DataFrame.                                          | `df.rename(columns={'viejo_nombre': 'nuevo_nombre'})`                                             |
| `value_counts()`     | Cuenta la cantidad de ocurrencias √∫nicas de valores en una serie o columna.                      | `df['columna'].value_counts()`                                                                    |
| `merge()`            | Combina dos DataFrames bas√°ndose en una clave com√∫n.                                             | `pd.merge(df1, df2, on='columna_clave')`                                                          |
| `pivot_table()`      | Crea una tabla din√°mica que resume los datos seg√∫n las variables especificadas.                  | `df.pivot_table(values='columna_valor', index='columna_indice', columns='columna_columnas')`      |
| `fillna()`           | Rellena los valores NaN (nulos) con un valor espec√≠fico.                                         | `df.fillna(valor=0)`                                                                              |
| `dropna()`           | Elimina las filas o columnas que contienen valores nulos.                                        | `df.dropna()`                                                                                     |
| `apply()`            | Aplica una funci√≥n a lo largo de un eje del DataFrame (filas o columnas).                        | `df['columna'].apply(funcion)`                                                                    |
| `astype()`           | Convierte el tipo de datos de una o m√°s columnas.                                                | `df['columna'] = df['columna'].astype(int)`                                                       |
| `set_index()`        | Establece una columna espec√≠fica como el √≠ndice del DataFrame.                                   | `df.set_index('columna')`                                                                         |
| `reset_index()`      | Restablece el √≠ndice del DataFrame al valor predeterminado.                                      | `df.reset_index()`                                                                                |
| `loc[]`              | Accede a un grupo de filas y columnas por etiquetas o una matriz booleana.                       | `df.loc[filas, columnas]`                                                                         |
| `iloc[]`             | Accede a un grupo de filas y columnas por √≠ndices enteros.                                       | `df.iloc[filas, columnas]`                                                                        |

